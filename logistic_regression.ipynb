{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/X_train.csv\")\n",
    "X_test = pd.read_csv(\"data/X_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"data/y_test.csv\").values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236866, 768) (236866,)\n"
     ]
    }
   ],
   "source": [
    "# implement SMOTE to oversample the minority class\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "os_X, os_y = os.fit_resample(X_train, y_train)\n",
    "print(os_X.shape, os_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the oversample data (keeping code for now in case we need it later)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# os_X_train, os_X_test, os_y_train, os_y_test = train_test_split(os_X, os_y, test_size=0.30, random_state=0)\n",
    "# print(os_X_train.shape, os_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Custom function to print the metrics of the model\n",
    "def print_metrics(y_test, y_pred):\n",
    "    print(f'Accuracy Score: {accuracy_score(y_test, y_pred)}')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check p-values of features\n",
    "# import statsmodels.api as sm\n",
    "# logit_model = sm.Logit(y_train, X_train)\n",
    "# result = logit_model.fit()\n",
    "# print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature with p-values less than 0.05 are the significant ones. Consider getting rid of insignificant features and then fit it into the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9130020973967605\n",
      "[[50469   226]\n",
      " [ 4710  1332]]\n",
      "Precision: 0.8549422336328626\n",
      "Recall: 0.22045680238331677\n",
      "F1 Score: 0.35052631578947363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brian\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=0, max_iter=200)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# cross_val_score(logreg, X_train, y_train, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# # define model\n",
    "# model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# # define cross-validation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# # evaluate model\n",
    "# scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "# # summarize performance\n",
    "# print('Mean AUROC: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression using oversampled data from SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8094365229039251\n",
      "[[41749  8946]\n",
      " [ 1866  4176]]\n",
      "Precision: 0.31824417009602196\n",
      "Recall: 0.6911618669314796\n",
      "F1 Score: 0.43581715716969316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brian\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_os = LogisticRegression(random_state=0, max_iter=200)\n",
    "logreg_os.fit(os_X, os_y)\n",
    "y_pred = logreg_os.predict(X_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use variance inflation factor to identify any significant multi-collinearity\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# X_constant = sm.add_constant(X_train, prepend=False)\n",
    "# vif = pd.DataFrame()\n",
    "# vif[\"Features\"] = X_constant.columns\n",
    "# vif[\"VIF\"] = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "# vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.25)\n",
    "# chis_train = pd.concat([X_train, y_train], axis=1)\n",
    "# chis_train = chis_train.rename(columns = {'T2D': 'Type 2 Diabetes', 'AB24': 'Taking Insulin', 'AB23_P1': 'Age Told To Have Diabetes', 'AB112': 'Diabetes Care Plan', \n",
    "# 'AB25': 'Diabetic Pills', 'AB63': 'Eye Exam Dilated Pupils', 'AB114_P1': 'Confidence to Control Diabetes', 'AB28_P1': 'Doctor Checked Feet for Sores', 'AB109': 'Visited ER', \n",
    "# 'AB113': 'Copy of Diabetes Care Plan', 'AB111': \"Hospital Overnight for Diabetes\"})\n",
    "\n",
    "# corrMatrix = chis_train.corr()\n",
    "# plt.subplots(figsize=(12, 9))\n",
    "# g = sns.heatmap(corrMatrix, annot=True, cmap=\"Accent\", vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Heatmap', size = 25)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.genmod.generalized_linear_model import GLM\n",
    "# from statsmodels.genmod import families\n",
    "\n",
    "# # Setup logistic regression model (using GLM method so that we can retrieve residuals)\n",
    "# logit_model = GLM(y_train, X_constant, family=families.Binomial())\n",
    "# logit_results = logit_model.fit()\n",
    "# print(logit_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# # Generate residual series plot\n",
    "# fig = plt.figure(figsize=(15,9))\n",
    "# ax = fig.add_subplot(111, title=\"Residual Series Plot\",\n",
    "#                     xlabel=\"Index Number\", ylabel=\"Deviance Residuals\")\n",
    "\n",
    "# # ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_pearson))\n",
    "# ax.plot(X_constant.index.tolist(), scipy.stats.zscore(logit_results.resid_deviance))\n",
    "# plt.axhline(y=0, ls=\"--\", color='red');"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae5651488652e961febbac79b6ac121d2261f82ccebdbef63236855c9fe5b051"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
